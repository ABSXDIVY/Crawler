#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å‘æ”¹å§”æ”¿ç­–æ•°æ®æå–æ¨¡å— - å®Œæ•´ç‰ˆæœ¬
ä»HTMLé¡µé¢ä¸­æå–æ”¿ç­–åˆ—è¡¨ã€æ­£æ–‡å†…å®¹ã€é™„ä»¶ä¿¡æ¯å’Œè§£è¯»ä¿¡æ¯
ä¿å­˜åˆ°Excelæ–‡ä»¶çš„å››ä¸ªå·¥ä½œè¡¨ä¸­
"""

import pandas as pd
import re
from bs4 import BeautifulSoup
from datetime import datetime
import os
import requests
import time
from urllib.parse import urljoin
import logging

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/data_extractor.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

class PolicyDataExtractor:
    """æ”¿ç­–æ•°æ®æå–å™¨ - å®Œæ•´ç‰ˆæœ¬"""
    
    def __init__(self, test_mode=False, max_test_items=10):
        """åˆå§‹åŒ–æå–å™¨"""
        self.policies_data = []  # æ”¿ç­–åˆ—è¡¨æ•°æ®
        self.interpretations_data = []  # è§£è¯»æ•°æ®
        self.content_data = []  # æ­£æ–‡å†…å®¹æ•°æ®
        self.attachments_data = []  # é™„ä»¶æ•°æ®
        self.base_url = 'https://www.ndrc.gov.cn'  # åŸºç¡€åŸŸå
        
        # æµ‹è¯•æ¨¡å¼è®¾ç½®
        self.test_mode = test_mode
        self.max_test_items = max_test_items
        self.processed_count = 0
        
        # åˆ›å»ºrequestsä¼šè¯
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        logging.info("æ”¿ç­–æ•°æ®æå–å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def get_page_content(self, url, retries=3):
        """è·å–é¡µé¢å†…å®¹"""
        for attempt in range(retries):
            try:
                logging.info(f"æ­£åœ¨è·å–é¡µé¢å†…å®¹: {url}")
                response = self.session.get(url, timeout=30)
                response.raise_for_status()
                response.encoding = 'utf-8'
                logging.info(f"æˆåŠŸè·å–é¡µé¢: {response.status_code}")
                return response.text
            except Exception as e:
                logging.warning(f"è·å–é¡µé¢å¤±è´¥ (å°è¯• {attempt + 1}/{retries}): {e}")
                if attempt < retries - 1:
                    time.sleep(2)
                else:
                    logging.error(f"æœ€ç»ˆè·å–å¤±è´¥: {url}")
                    return None
    
    def extract_policy_info(self, html_content, category_name, page_num):
        """ä»HTMLä¸­æå–æ”¿ç­–ä¿¡æ¯"""
        if not html_content:
            return
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # æŸ¥æ‰¾æ‰€æœ‰æ”¿ç­–åˆ—è¡¨é¡¹
        policy_items = soup.find_all('li')
        
        for item in policy_items:
            # æµ‹è¯•æ¨¡å¼æ£€æŸ¥
            if self.test_mode and self.processed_count >= self.max_test_items:
                logging.info(f"æµ‹è¯•æ¨¡å¼ï¼šå·²è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ {self.max_test_items}")
                return
            
            try:
                # æŸ¥æ‰¾æ”¿ç­–é“¾æ¥
                policy_link = item.find('a', href=True)
                if not policy_link:
                    continue
                
                # æ£€æŸ¥æ˜¯å¦æ˜¯æ”¿ç­–é“¾æ¥ï¼ˆåŒ…å«æ—¥æœŸä¿¡æ¯ï¼‰
                date_span = item.find('span')
                if not date_span:
                    continue
                
                # æå–æ”¿ç­–æ ‡é¢˜
                title = policy_link.get('title', '').strip()
                if not title:
                    title = policy_link.get_text(strip=True)
                
                # æå–é“¾æ¥å¹¶è½¬æ¢ä¸ºå®Œæ•´URL
                href = policy_link.get('href', '')
                if href.startswith('./'):
                    href = href[2:]  # ç§»é™¤å¼€å¤´çš„ ./
                
                # æ„å»ºå®Œæ•´URL
                full_url = self.build_full_url(href, category_name)
                
                # æå–å‘å¸ƒæ—¥æœŸ
                publish_date = date_span.get_text(strip=True) if date_span else ''
                
                # æå–æ–‡å·ï¼ˆä»æ ‡é¢˜ä¸­æå–ï¼‰
                document_number = self.extract_document_number(title)
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è§£è¯»
                has_interpretation = self.check_has_interpretation(item)
                
                # æå–è§£è¯»ä¿¡æ¯
                interpretations = self.extract_interpretations(item, href)
                
                # è·å–æ”¿ç­–è¯¦æƒ…é¡µé¢çš„æ­£æ–‡å†…å®¹å’Œé™„ä»¶ä¿¡æ¯
                content_info = self.extract_policy_detail(full_url, title)
                
                # æ·»åŠ åˆ°æ”¿ç­–æ•°æ®
                policy_data = {
                    'æ”¿ç­–åˆ†ç±»': category_name,
                    'é¡µç ': page_num,
                    'æ”¿ç­–æ ‡é¢˜': title,
                    'æ–‡å·': document_number,
                    'å‘å¸ƒæ—¥æœŸ': publish_date,
                    'æ”¿ç­–é“¾æ¥': full_url,
                    'æ˜¯å¦æœ‰è§£è¯»': has_interpretation,
                    'è§£è¯»æ•°é‡': len(interpretations)
                }
                
                self.policies_data.append(policy_data)
                
                # æ·»åŠ æ­£æ–‡å†…å®¹æ•°æ®
                if content_info.get('content'):
                    content_data = {
                        'æ”¿ç­–åˆ†ç±»': category_name,
                        'æ”¿ç­–æ ‡é¢˜': title,
                        'æ–‡å·': document_number,
                        'å‘å¸ƒæ—¥æœŸ': publish_date,
                        'æ”¿ç­–é“¾æ¥': full_url,
                        'æ­£æ–‡å†…å®¹': content_info['content']
                    }
                    self.content_data.append(content_data)
                
                # æ·»åŠ é™„ä»¶æ•°æ®
                if content_info.get('attachments') or content_info.get('attachment_links'):
                    attachment_data = {
                        'æ”¿ç­–åˆ†ç±»': category_name,
                        'æ”¿ç­–æ ‡é¢˜': title,
                        'æ–‡å·': document_number,
                        'å‘å¸ƒæ—¥æœŸ': publish_date,
                        'æ”¿ç­–é“¾æ¥': full_url,
                        'é™„ä»¶ä¿¡æ¯': content_info.get('attachments', ''),
                        'é™„ä»¶é“¾æ¥': content_info.get('attachment_links', '')
                    }
                    self.attachments_data.append(attachment_data)
                
                # æ·»åŠ è§£è¯»æ•°æ®
                for interpretation in interpretations:
                    interpretation_data = {
                        'æ”¿ç­–åˆ†ç±»': category_name,
                        'æ”¿ç­–æ ‡é¢˜': title,
                        'æ”¿ç­–æ—¥æœŸ': publish_date,
                        'æ”¿ç­–é“¾æ¥': full_url,
                        'è§£è¯»æ ‡é¢˜': interpretation['title'],
                        'è§£è¯»é“¾æ¥': interpretation['full_url']
                    }
                    self.interpretations_data.append(interpretation_data)
                
                # å¢åŠ å¤„ç†è®¡æ•°
                self.processed_count += 1
                
                logging.info(f"å·²å¤„ç†æ”¿ç­–: {title}")
                
                # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
                time.sleep(1)
                
            except Exception as e:
                logging.error(f"æå–æ”¿ç­–ä¿¡æ¯æ—¶å‡ºé”™: {e}")
                continue
    
    def extract_policy_detail(self, url, title):
        """æå–æ”¿ç­–è¯¦æƒ…é¡µé¢çš„æ­£æ–‡å†…å®¹å’Œé™„ä»¶ä¿¡æ¯"""
        try:
            html_content = self.get_page_content(url)
            if not html_content:
                return {'content': '', 'attachments': '', 'attachment_links': ''}
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # æå–æ­£æ–‡å†…å®¹
            content = self.extract_content(soup)
            
            # æå–é™„ä»¶ä¿¡æ¯
            attachments_info = self.extract_attachments(soup, url)
            
            return {
                'content': content,
                'attachments': attachments_info['attachments'],
                'attachment_links': attachments_info['links']
            }
            
        except Exception as e:
            logging.error(f"æå–æ”¿ç­–è¯¦æƒ…æ—¶å‡ºé”™: {e}")
            return {'content': '', 'attachments': '', 'attachment_links': ''}
    
    def extract_content(self, soup):
        """æå–æ­£æ–‡å†…å®¹"""
        try:
            # ä¼˜å…ˆæŸ¥æ‰¾ç‰¹å®šçš„æ–‡ç« å†…å®¹å®¹å™¨
            content_element = soup.find('div', class_='article_con')
            
            if not content_element:
                # å¦‚æœæ‰¾ä¸åˆ°ç‰¹å®šå®¹å™¨ï¼Œå°è¯•å…¶ä»–é€‰æ‹©å™¨
                content_selectors = [
                    '.TRS_Editor',  # å‘æ”¹å§”ç½‘ç«™å¸¸ç”¨çš„ç¼–è¾‘å™¨å®¹å™¨
                    '.content', '.article-content', '.main-content',
                    '.policy-content', '.document-content', '.text-content',
                    '.article', '.text', '.main'
                ]
                
                for selector in content_selectors:
                    content_element = soup.select_one(selector)
                    if content_element:
                        break
            
            if content_element:
                # æ¸…ç†å†…å®¹
                content_text = self.clean_content(content_element.get_text())
                # é™åˆ¶å†…å®¹é•¿åº¦ï¼Œé¿å…Excelå•å…ƒæ ¼è¿‡å¤§
                return content_text[:15000] if len(content_text) > 15000 else content_text
            
            return ''
            
        except Exception as e:
            logging.error(f"æå–æ­£æ–‡å†…å®¹æ—¶å‡ºé”™: {e}")
            return ''
    
    def extract_attachments(self, soup, base_url):
        """æå–é™„ä»¶ä¿¡æ¯"""
        try:
            attachments = []
            attachment_links = []
            
            # æŸ¥æ‰¾é™„ä»¶é“¾æ¥ - æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼
            attachment_selectors = [
                'a[href*=".pdf"]',
                'a[href*=".doc"]',
                'a[href*=".docx"]',
                'a[href*=".ofd"]',
                'a[href*=".xls"]',
                'a[href*=".xlsx"]',
                'a[href*=".zip"]',
                'a[href*=".rar"]'
            ]
            
            for selector in attachment_selectors:
                links = soup.select(selector)
                for link in links:
                    href = link.get('href', '')
                    text = link.get_text(strip=True)
                    
                    if href:
                        # æ„å»ºå®Œæ•´é“¾æ¥
                        if href.startswith('http'):
                            full_url = href
                        else:
                            full_url = urljoin(base_url, href)
                        
                        attachments.append(text if text else href)
                        attachment_links.append(full_url)
            
            return {
                'attachments': '; '.join(attachments),
                'links': '; '.join(attachment_links)
            }
            
        except Exception as e:
            logging.error(f"æå–é™„ä»¶ä¿¡æ¯æ—¶å‡ºé”™: {e}")
            return {'attachments': '', 'links': ''}
    
    def clean_content(self, text):
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        if not text:
            return ''
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        text = re.sub(r'\s+', ' ', text.strip())
        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦
        text = re.sub(r'[\r\n\t]+', '\n', text)
        return text
    
    def build_full_url(self, href, category_name):
        """æ„å»ºå®Œæ•´çš„URLï¼ŒåŒ…å«æ­£ç¡®çš„ç›®å½•è·¯å¾„"""
        # æ ¹æ®åˆ†ç±»ç¡®å®šåŸºç¡€è·¯å¾„
        category_paths = {
            'å‘å±•æ”¹é©å§”ä»¤': '/xxgk/zcfb/fzggwl',
            'è§„èŒƒæ€§æ–‡ä»¶': '/xxgk/zcfb/ghxwj',
            'è§„åˆ’æ–‡æœ¬': '/xxgk/zcfb/ghwb',
            'å…¬å‘Š': '/xxgk/zcfb/gg',
            'é€šçŸ¥': '/xxgk/zcfb/tz'
        }
        
        base_path = category_paths.get(category_name, '')
        if base_path and not href.startswith('http'):
            # æ„å»ºå®Œæ•´URL
            full_url = f"{self.base_url}{base_path}/{href}"
        else:
            # å¦‚æœå·²ç»æ˜¯å®Œæ•´URLæˆ–æ— æ³•ç¡®å®šè·¯å¾„ï¼Œç›´æ¥æ‹¼æ¥
            full_url = urljoin(self.base_url, href)
        
        return full_url
    
    def check_has_interpretation(self, item):
        """æ£€æŸ¥æ˜¯å¦æœ‰è§£è¯»ä¿¡æ¯"""
        # å¤šç§æ–¹å¼æ£€æŸ¥è§£è¯»æ ‡è¯†
        indicators = [
            item.find('img', src='/images/jiedu.png'),
            item.find('img', src=lambda x: x and 'jiedu' in x),
            item.find('div', class_='popbox'),
            item.find('strong', recursive=True),  # è§£è¯»å›¾æ ‡é€šå¸¸è¢«strongæ ‡ç­¾åŒ…å›´
        ]
        
        return any(indicators)
    
    def extract_document_number(self, title):
        """ä»æ ‡é¢˜ä¸­æå–æ–‡å·"""
        # åŒ¹é…å¸¸è§çš„æ–‡å·æ ¼å¼
        patterns = [
            r'ç¬¬(\d+)å·ä»¤',
            r'ç¬¬(\d+)å·',
            r'(\d+)å·ä»¤',
            r'(\d+)å·'
        ]
        
        for pattern in patterns:
            match = re.search(pattern, title)
            if match:
                return match.group(0)
        
        return ''
    
    def extract_interpretations(self, item, policy_url):
        """æå–è§£è¯»ä¿¡æ¯"""
        interpretations = []
        
        try:
            # æŸ¥æ‰¾è§£è¯»å¼¹çª— - å¤šç§æ–¹å¼æŸ¥æ‰¾
            popbox = item.find('div', class_='popbox')
            
            # å¦‚æœæ‰¾ä¸åˆ°popboxï¼Œå°è¯•å…¶ä»–æ–¹å¼æŸ¥æ‰¾è§£è¯»ä¿¡æ¯
            if not popbox:
                # æŸ¥æ‰¾è§£è¯»å›¾æ ‡å‘¨å›´çš„è§£è¯»é“¾æ¥
                interpretation_indicators = [
                    item.find('img', src='/images/jiedu.png'),
                    item.find('img', src=lambda x: x and 'jiedu' in x),
                    item.find('strong', recursive=True)
                ]
                
                for indicator in interpretation_indicators:
                    if indicator:
                        # åœ¨è§£è¯»å›¾æ ‡é™„è¿‘æŸ¥æ‰¾é“¾æ¥
                        nearby_links = indicator.find_next_siblings('a', href=True)
                        if not nearby_links:
                            nearby_links = indicator.parent.find_all('a', href=True)
                        
                        for link in nearby_links:
                            title = link.get('title', '').strip()
                            if not title:
                                title = link.get_text(strip=True)
                            
                            href = link.get('href', '')
                            if href.startswith('../../'):
                                href = href[6:]  # ç§»é™¤å¼€å¤´çš„ ../../
                            
                            # æ„å»ºå®Œæ•´URL - è§£è¯»é“¾æ¥é€šå¸¸æŒ‡å‘jdç›®å½•
                            if href.startswith('jd/'):
                                full_url = f"{self.base_url}/xxgk/{href}"
                            else:
                                full_url = urljoin(self.base_url, href)
                            
                            interpretations.append({
                                'title': title,
                                'url': href,
                                'full_url': full_url
                            })
                        break
            else:
                # æŸ¥æ‰¾è§£è¯»åˆ—è¡¨
                interpretation_links = popbox.find_all('a', href=True)
                
                for link in interpretation_links:
                    title = link.get('title', '').strip()
                    if not title:
                        title = link.get_text(strip=True)
                    
                    href = link.get('href', '')
                    if href.startswith('../../'):
                        href = href[6:]  # ç§»é™¤å¼€å¤´çš„ ../../
                    
                    # æ„å»ºå®Œæ•´URL - è§£è¯»é“¾æ¥é€šå¸¸æŒ‡å‘jdç›®å½•
                    if href.startswith('jd/'):
                        full_url = f"{self.base_url}/xxgk/{href}"
                    else:
                        full_url = urljoin(self.base_url, href)
                    
                    interpretations.append({
                        'title': title,
                        'url': href,
                        'full_url': full_url
                    })
                
        except Exception as e:
            logging.error(f"æå–è§£è¯»ä¿¡æ¯æ—¶å‡ºé”™: {e}")
        
        return interpretations
    
    def save_to_excel(self, output_file):
        """ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶ - å››ä¸ªå·¥ä½œè¡¨"""
        try:
            logging.info("å¼€å§‹ä¿å­˜æ•°æ®åˆ°Excelæ–‡ä»¶")
            
            # åˆ›å»ºExcelå†™å…¥å™¨
            with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
                # å®šä¹‰ç›®å½•å¤„ç†é¡ºåº
                category_order = ['å‘å±•æ”¹é©å§”ä»¤', 'è§„èŒƒæ€§æ–‡ä»¶', 'è§„åˆ’æ–‡æœ¬', 'å…¬å‘Š', 'é€šçŸ¥']
                
                # ===== Sheet1: æ”¿ç­–åˆ—è¡¨ =====
                if self.policies_data:
                    policies_df = pd.DataFrame(self.policies_data)
                    
                    # æŒ‰ç…§ç›®å½•å’Œé¡µé¢é¡ºåºæ’åº
                    policies_df['category_order'] = policies_df['æ”¿ç­–åˆ†ç±»'].map(lambda x: category_order.index(x) if x in category_order else 999)
                    policies_df = policies_df.sort_values(['category_order', 'é¡µç ', 'å‘å¸ƒæ—¥æœŸ'], ascending=[True, True, False])
                    policies_df = policies_df.drop('category_order', axis=1)
                    
                    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
                    column_order = [
                        'æ”¿ç­–åˆ†ç±»', 'é¡µç ', 'æ”¿ç­–æ ‡é¢˜', 'æ–‡å·', 
                        'å‘å¸ƒæ—¥æœŸ', 'æ”¿ç­–é“¾æ¥', 'æ˜¯å¦æœ‰è§£è¯»', 'è§£è¯»æ•°é‡'
                    ]
                    existing_columns = [col for col in column_order if col in policies_df.columns]
                    policies_df = policies_df[existing_columns]
                    
                    policies_df.to_excel(writer, sheet_name='æ”¿ç­–åˆ—è¡¨', index=False)
                    logging.info(f"âœ… Sheet1 - æ”¿ç­–åˆ—è¡¨: å·²ä¿å­˜{len(self.policies_data)}æ¡è®°å½•")
                
                # ===== Sheet2: æ”¿ç­–æ­£æ–‡ =====
                if self.content_data:
                    content_df = pd.DataFrame(self.content_data)
                    
                    # æŒ‰ç…§ç›®å½•å’Œé¡µé¢é¡ºåºæ’åº
                    content_df['category_order'] = content_df['æ”¿ç­–åˆ†ç±»'].map(lambda x: category_order.index(x) if x in category_order else 999)
                    content_df = content_df.sort_values(['category_order', 'å‘å¸ƒæ—¥æœŸ'], ascending=[True, False])
                    content_df = content_df.drop('category_order', axis=1)
                    
                    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
                    column_order = [
                        'æ”¿ç­–åˆ†ç±»', 'æ”¿ç­–æ ‡é¢˜', 'æ–‡å·', 'å‘å¸ƒæ—¥æœŸ', 
                        'æ”¿ç­–é“¾æ¥', 'æ­£æ–‡å†…å®¹'
                    ]
                    existing_columns = [col for col in column_order if col in content_df.columns]
                    content_df = content_df[existing_columns]
                    
                    content_df.to_excel(writer, sheet_name='æ”¿ç­–æ­£æ–‡', index=False)
                    logging.info(f"âœ… Sheet2 - æ”¿ç­–æ­£æ–‡: å·²ä¿å­˜{len(self.content_data)}æ¡è®°å½•")
                
                # ===== Sheet3: æ”¿ç­–é™„ä»¶ =====
                if self.attachments_data:
                    # å¤„ç†é™„ä»¶æ•°æ®ï¼Œå°†ä¸åŒç±»å‹çš„é™„ä»¶åˆ†å¼€
                    processed_attachments = []
                    
                    for attachment_data in self.attachments_data:
                        attachments = attachment_data.get('é™„ä»¶ä¿¡æ¯', '').split('; ')
                        attachment_links = attachment_data.get('é™„ä»¶é“¾æ¥', '').split('; ')
                        
                        # æŒ‰æ–‡ä»¶ç±»å‹åˆ†ç±»
                        file_types = {
                            'PDF': {'attachments': [], 'links': []},
                            'OFD': {'attachments': [], 'links': []},
                            'Word': {'attachments': [], 'links': []},
                            'Excel': {'attachments': [], 'links': []},
                            'å…¶ä»–': {'attachments': [], 'links': []}
                        }
                        
                        for i, (attachment, link) in enumerate(zip(attachments, attachment_links)):
                            if attachment and link:
                                link_lower = link.lower()
                                if link_lower.endswith('.pdf'):
                                    file_types['PDF']['attachments'].append(attachment)
                                    file_types['PDF']['links'].append(link)
                                elif link_lower.endswith('.ofd'):
                                    file_types['OFD']['attachments'].append(attachment)
                                    file_types['OFD']['links'].append(link)
                                elif link_lower.endswith(('.doc', '.docx')):
                                    file_types['Word']['attachments'].append(attachment)
                                    file_types['Word']['links'].append(link)
                                elif link_lower.endswith(('.xls', '.xlsx')):
                                    file_types['Excel']['attachments'].append(attachment)
                                    file_types['Excel']['links'].append(link)
                                else:
                                    file_types['å…¶ä»–']['attachments'].append(attachment)
                                    file_types['å…¶ä»–']['links'].append(link)
                        
                        # ä¸ºæ¯ä¸ªé™„ä»¶ç±»å‹åˆ›å»ºä¸€è¡Œè®°å½•
                        for file_type, data in file_types.items():
                            if data['attachments']:
                                processed_attachments.append({
                                    'æ”¿ç­–åˆ†ç±»': attachment_data['æ”¿ç­–åˆ†ç±»'],
                                    'æ”¿ç­–æ ‡é¢˜': attachment_data['æ”¿ç­–æ ‡é¢˜'],
                                    'æ–‡å·': attachment_data['æ–‡å·'],
                                    'å‘å¸ƒæ—¥æœŸ': attachment_data['å‘å¸ƒæ—¥æœŸ'],
                                    'æ”¿ç­–é“¾æ¥': attachment_data['æ”¿ç­–é“¾æ¥'],
                                    'é™„ä»¶ç±»å‹': file_type,
                                    'é™„ä»¶åç§°': '\n'.join(data['attachments']),
                                    'é™„ä»¶é“¾æ¥': '\n'.join(data['links'])
                                })
                    
                    if processed_attachments:
                        attachments_df = pd.DataFrame(processed_attachments)
                        
                        # æŒ‰ç…§ç›®å½•å’Œé¡µé¢é¡ºåºæ’åº
                        attachments_df['category_order'] = attachments_df['æ”¿ç­–åˆ†ç±»'].map(lambda x: category_order.index(x) if x in category_order else 999)
                        attachments_df = attachments_df.sort_values(['category_order', 'å‘å¸ƒæ—¥æœŸ'], ascending=[True, False])
                        attachments_df = attachments_df.drop('category_order', axis=1)
                        
                        # é‡æ–°æ’åˆ—åˆ—é¡ºåº
                        column_order = [
                            'æ”¿ç­–åˆ†ç±»', 'æ”¿ç­–æ ‡é¢˜', 'æ–‡å·', 'å‘å¸ƒæ—¥æœŸ', 
                            'æ”¿ç­–é“¾æ¥', 'é™„ä»¶ç±»å‹', 'é™„ä»¶åç§°', 'é™„ä»¶é“¾æ¥'
                        ]
                        existing_columns = [col for col in column_order if col in attachments_df.columns]
                        attachments_df = attachments_df[existing_columns]
                        
                        attachments_df.to_excel(writer, sheet_name='æ”¿ç­–é™„ä»¶', index=False)
                        logging.info(f"âœ… Sheet3 - æ”¿ç­–é™„ä»¶: å·²ä¿å­˜{len(processed_attachments)}æ¡è®°å½•")
                
                # ===== Sheet4: æ”¿ç­–è§£è¯» =====
                if self.interpretations_data:
                    # ç›´æ¥ä½¿ç”¨åŸå§‹è§£è¯»æ•°æ®ï¼Œæ¯ä¸ªè§£è¯»å•ç‹¬ä¸€è¡Œ
                    interpretations_df = pd.DataFrame(self.interpretations_data)
                    
                    # æŒ‰ç…§ç›®å½•å’Œé¡µé¢é¡ºåºæ’åº
                    interpretations_df['category_order'] = interpretations_df['æ”¿ç­–åˆ†ç±»'].map(lambda x: category_order.index(x) if x in category_order else 999)
                    interpretations_df = interpretations_df.sort_values(['category_order', 'æ”¿ç­–æ—¥æœŸ'], ascending=[True, False])
                    interpretations_df = interpretations_df.drop('category_order', axis=1)
                    
                    # é‡æ–°æ’åˆ—åˆ—é¡ºåº
                    column_order = [
                        'æ”¿ç­–åˆ†ç±»', 'æ”¿ç­–æ ‡é¢˜', 'æ”¿ç­–æ—¥æœŸ', 'æ”¿ç­–é“¾æ¥',
                        'è§£è¯»æ ‡é¢˜', 'è§£è¯»é“¾æ¥'
                    ]
                    existing_columns = [col for col in column_order if col in interpretations_df.columns]
                    interpretations_df = interpretations_df[existing_columns]
                    
                    interpretations_df.to_excel(writer, sheet_name='æ”¿ç­–è§£è¯»', index=False)
                    logging.info(f"âœ… Sheet4 - æ”¿ç­–è§£è¯»: å·²ä¿å­˜{len(self.interpretations_data)}æ¡è®°å½•")
            
            logging.info(f"ğŸ‰ æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°: {output_file}")
            
        except Exception as e:
            logging.error(f"ä¿å­˜Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
            raise
    
    def get_statistics(self):
        """è·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'total_policies': len(self.policies_data),
            'total_interpretations': len(self.interpretations_data),
            'total_contents': len(self.content_data),
            'total_attachments': len(self.attachments_data),
            'policies_with_interpretations': len([p for p in self.policies_data if p['æ˜¯å¦æœ‰è§£è¯»']]),
            'categories': list(set([p['æ”¿ç­–åˆ†ç±»'] for p in self.policies_data]))
        }
        
        return stats

def process_html_files(html_dir, output_file, test_mode=False, max_test_items=10):
    """å¤„ç†HTMLæ–‡ä»¶å¹¶æå–æ•°æ®"""
    logging.info("å¼€å§‹å¤„ç†HTMLæ–‡ä»¶")
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    os.makedirs('logs', exist_ok=True)
    
    extractor = PolicyDataExtractor(test_mode=test_mode, max_test_items=max_test_items)
    
    # å®šä¹‰ç›®å½•å¤„ç†é¡ºåº
    category_order = ['å‘å±•æ”¹é©å§”ä»¤', 'è§„èŒƒæ€§æ–‡ä»¶', 'è§„åˆ’æ–‡æœ¬', 'å…¬å‘Š', 'é€šçŸ¥']
    
    # æŒ‰ç…§æŒ‡å®šé¡ºåºå¤„ç†ç›®å½•
    for category_name in category_order:
        category_path = os.path.join(html_dir, category_name)
        
        if not os.path.exists(category_path):
            logging.warning(f"ç›®å½•ä¸å­˜åœ¨: {category_name}")
            continue
        
        logging.info(f"ğŸ“ å¤„ç†åˆ†ç±»: {category_name}")
        
        # è·å–è¯¥åˆ†ç±»ä¸‹çš„æ‰€æœ‰HTMLæ–‡ä»¶
        html_files = []
        for filename in os.listdir(category_path):
            if not filename.endswith('.html'):
                continue
            
            # ä»æ–‡ä»¶åä¸­æå–é¡µç  - æ”¯æŒå¤šç§æ ¼å¼
            page_match = re.search(r'page_(\d+)', filename)
            if not page_match:
                continue
            
            page_num = int(page_match.group(1))
            file_path = os.path.join(category_path, filename)
            html_files.append((page_num, file_path, filename))
        
        # æŒ‰ç…§é¡µç æ’åº
        html_files.sort(key=lambda x: x[0])
        
        logging.info(f"æ‰¾åˆ° {len(html_files)} ä¸ªHTMLæ–‡ä»¶")
        
        # å¤„ç†æ’åºåçš„æ–‡ä»¶
        for page_num, file_path, filename in html_files:
            try:
                logging.info(f"å¤„ç†æ–‡ä»¶: {filename}")
                
                # è¯»å–HTMLæ–‡ä»¶
                with open(file_path, 'r', encoding='utf-8') as f:
                    html_content = f.read()
                
                # æå–æ•°æ®
                extractor.extract_policy_info(html_content, category_name, page_num)
                
                # æµ‹è¯•æ¨¡å¼æ£€æŸ¥
                if test_mode and extractor.processed_count >= max_test_items:
                    logging.info(f"æµ‹è¯•æ¨¡å¼ï¼šå·²è¾¾åˆ°æœ€å¤§å¤„ç†æ•°é‡ {max_test_items}")
                    break
                
            except Exception as e:
                logging.error(f"å¤„ç†æ–‡ä»¶ {filename} æ—¶å‡ºé”™: {e}")
        
        # æµ‹è¯•æ¨¡å¼æ£€æŸ¥
        if test_mode and extractor.processed_count >= max_test_items:
            break
    
    # ä¿å­˜æ•°æ®åˆ°Excel
    extractor.save_to_excel(output_file)
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = extractor.get_statistics()
    logging.info("\nğŸ“Š æ•°æ®æå–ç»Ÿè®¡:")
    logging.info(f"æ€»æ”¿ç­–æ•°: {stats['total_policies']}")
    logging.info(f"æ€»è§£è¯»æ•°: {stats['total_interpretations']}")
    logging.info(f"æ€»æ­£æ–‡æ•°: {stats['total_contents']}")
    logging.info(f"æ€»é™„ä»¶æ•°: {stats['total_attachments']}")
    logging.info(f"æœ‰è§£è¯»çš„æ”¿ç­–æ•°: {stats['policies_with_interpretations']}")
    logging.info(f"æ¶‰åŠåˆ†ç±»: {', '.join(stats['categories'])}")
    
    return extractor

if __name__ == "__main__":
    # å®Œæ•´æ¨¡å¼ï¼šå¤„ç†æ‰€æœ‰æ”¿ç­–
    logging.info("ğŸš€ å¯åŠ¨æ•°æ®æå–å™¨ - å®Œæ•´æ¨¡å¼")
    process_html_files('results', 'policy_data_full.xlsx', test_mode=False)
    
    # æµ‹è¯•æ¨¡å¼ï¼šåªå¤„ç†å‰10ä¸ªæ”¿ç­–
    # logging.info("ğŸš€ å¯åŠ¨æ•°æ®æå–å™¨ - æµ‹è¯•æ¨¡å¼")
    # process_html_files('results', 'policy_data_test.xlsx', test_mode=True, max_test_items=10)
